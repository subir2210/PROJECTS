{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Subir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Subir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Subir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('C:/Users/Subir/Learn Bay/Learn Bay Local Copy/Dataset_Backup/SMSSpamCollection', sep='\\t', names=['Labels','Messages'])\n",
    "# For testing purpose used 99 records/rows.\n",
    "df=df.iloc[0:99,:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Labels                                           Messages\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word Tokenization --> ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
      "lemmatization --> ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n",
      "\n",
      "word Tokenization --> ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "lemmatization --> ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "\n",
      "word Tokenization --> ['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n",
      "lemmatization --> ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply']\n",
      "\n",
      "word Tokenization --> ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n",
      "lemmatization --> ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']\n",
      "\n",
      "word Tokenization --> ['nah', 'i', 'don', 't', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']\n",
      "lemmatization --> ['nah', 'think', 'go', 'usf', 'life', 'around', 'though']\n",
      "\n",
      "word Tokenization --> ['freemsg', 'hey', 'there', 'darling', 'it', 's', 'been', 'week', 's', 'now', 'and', 'no', 'word', 'back', 'i', 'd', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'to', 'send', 'to', 'rcv']\n",
      "lemmatization --> ['freemsg', 'hey', 'darling', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'rcv']\n",
      "\n",
      "word Tokenization --> ['even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me', 'they', 'treat', 'me', 'like', 'aids', 'patent']\n",
      "lemmatization --> ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent']\n",
      "\n",
      "word Tokenization --> ['as', 'per', 'your', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune']\n",
      "lemmatization --> ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'caller', 'press', 'copy', 'friend', 'callertune']\n",
      "\n",
      "word Tokenization --> ['winner', 'as', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'prize', 'reward', 'to', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours', 'only']\n",
      "lemmatization --> ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hour']\n",
      "\n",
      "word Tokenization --> ['had', 'your', 'mobile', 'months', 'or', 'more', 'u', 'r', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
      "lemmatization --> ['mobile', 'month', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobile', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
      "\n",
      "word Tokenization --> ['i', 'm', 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'don', 't', 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', 'k', 'i', 've', 'cried', 'enough', 'today']\n",
      "lemmatization --> ['gon', 'na', 'home', 'soon', 'want', 'talk', 'stuff', 'anymore', 'tonight', 'k', 'cried', 'enough', 'today']\n",
      "\n",
      "word Tokenization --> ['six', 'chances', 'to', 'win', 'cash', 'from', 'to', 'pounds', 'txt', 'csh', 'and', 'send', 'to', 'cost', 'p', 'day', 'days', 'tsandcs', 'apply', 'reply', 'hl', 'info']\n",
      "lemmatization --> ['six', 'chance', 'win', 'cash', 'pound', 'txt', 'csh', 'send', 'cost', 'p', 'day', 'day', 'tsandcs', 'apply', 'reply', 'hl', 'info']\n",
      "\n",
      "word Tokenization --> ['urgent', 'you', 'have', 'won', 'a', 'week', 'free', 'membership', 'in', 'our', 'prize', 'jackpot', 'txt', 'the', 'word', 'claim', 'to', 'no', 't', 'c', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw', 'a', 'rw']\n",
      "lemmatization --> ['urgent', 'week', 'free', 'membership', 'prize', 'jackpot', 'txt', 'word', 'claim', 'c', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw', 'rw']\n",
      "\n",
      "word Tokenization --> ['i', 've', 'been', 'searching', 'for', 'the', 'right', 'words', 'to', 'thank', 'you', 'for', 'this', 'breather', 'i', 'promise', 'i', 'wont', 'take', 'your', 'help', 'for', 'granted', 'and', 'will', 'fulfil', 'my', 'promise', 'you', 'have', 'been', 'wonderful', 'and', 'a', 'blessing', 'at', 'all', 'times']\n",
      "lemmatization --> ['searching', 'right', 'word', 'thank', 'breather', 'promise', 'wont', 'take', 'help', 'granted', 'fulfil', 'promise', 'wonderful', 'blessing', 'time']\n",
      "\n",
      "word Tokenization --> ['i', 'have', 'a', 'date', 'on', 'sunday', 'with', 'will']\n",
      "lemmatization --> ['date', 'sunday']\n",
      "\n",
      "word Tokenization --> ['xxxmobilemovieclub', 'to', 'use', 'your', 'credit', 'click', 'the', 'wap', 'link', 'in', 'the', 'next', 'txt', 'message', 'or', 'click', 'here', 'http', 'wap', 'xxxmobilemovieclub', 'com', 'n', 'qjkgighjjgcbl']\n",
      "lemmatization --> ['xxxmobilemovieclub', 'use', 'credit', 'click', 'wap', 'link', 'next', 'txt', 'message', 'click', 'http', 'wap', 'xxxmobilemovieclub', 'com', 'n', 'qjkgighjjgcbl']\n",
      "\n",
      "word Tokenization --> ['oh', 'k', 'i', 'm', 'watching', 'here']\n",
      "lemmatization --> ['oh', 'k', 'watching']\n",
      "\n",
      "word Tokenization --> ['eh', 'u', 'remember', 'how', 'spell', 'his', 'name', 'yes', 'i', 'did', 'he', 'v', 'naughty', 'make', 'until', 'i', 'v', 'wet']\n",
      "lemmatization --> ['eh', 'u', 'remember', 'spell', 'name', 'yes', 'v', 'naughty', 'make', 'v', 'wet']\n",
      "\n",
      "word Tokenization --> ['fine', 'if', 'that', 's', 'the', 'way', 'u', 'feel', 'that', 's', 'the', 'way', 'its', 'gota', 'b']\n",
      "lemmatization --> ['fine', 'way', 'u', 'feel', 'way', 'gota', 'b']\n",
      "\n",
      "word Tokenization --> ['england', 'v', 'macedonia', 'dont', 'miss', 'the', 'goals', 'team', 'news', 'txt', 'ur', 'national', 'team', 'to', 'eg', 'england', 'to', 'try', 'wales', 'scotland', 'txt', 'poboxox', 'w', 'wq']\n",
      "lemmatization --> ['england', 'v', 'macedonia', 'dont', 'miss', 'goal', 'team', 'news', 'txt', 'ur', 'national', 'team', 'eg', 'england', 'try', 'wale', 'scotland', 'txt', 'poboxox', 'w', 'wq']\n",
      "\n",
      "word Tokenization --> ['is', 'that', 'seriously', 'how', 'you', 'spell', 'his', 'name']\n",
      "lemmatization --> ['seriously', 'spell', 'name']\n",
      "\n",
      "word Tokenization --> ['i', 'm', 'going', 'to', 'try', 'for', 'months', 'ha', 'ha', 'only', 'joking']\n",
      "lemmatization --> ['going', 'try', 'month', 'ha', 'ha', 'joking']\n",
      "\n",
      "word Tokenization --> ['so', 'pay', 'first', 'lar', 'then', 'when', 'is', 'da', 'stock', 'comin']\n",
      "lemmatization --> ['pay', 'first', 'lar', 'da', 'stock', 'comin']\n",
      "\n",
      "word Tokenization --> ['aft', 'i', 'finish', 'my', 'lunch', 'then', 'i', 'go', 'str', 'down', 'lor', 'ard', 'smth', 'lor', 'u', 'finish', 'ur', 'lunch', 'already']\n",
      "lemmatization --> ['aft', 'finish', 'lunch', 'go', 'str', 'lor', 'ard', 'smth', 'lor', 'u', 'finish', 'ur', 'lunch', 'already']\n",
      "\n",
      "word Tokenization --> ['ffffffffff', 'alright', 'no', 'way', 'i', 'can', 'meet', 'up', 'with', 'you', 'sooner']\n",
      "lemmatization --> ['ffffffffff', 'alright', 'way', 'meet', 'sooner']\n",
      "\n",
      "word Tokenization --> ['just', 'forced', 'myself', 'to', 'eat', 'a', 'slice', 'i', 'm', 'really', 'not', 'hungry', 'tho', 'this', 'sucks', 'mark', 'is', 'getting', 'worried', 'he', 'knows', 'i', 'm', 'sick', 'when', 'i', 'turn', 'down', 'pizza', 'lol']\n",
      "lemmatization --> ['forced', 'eat', 'slice', 'really', 'hungry', 'tho', 'suck', 'mark', 'getting', 'worried', 'know', 'sick', 'turn', 'pizza', 'lol']\n",
      "\n",
      "word Tokenization --> ['lol', 'your', 'always', 'so', 'convincing']\n",
      "lemmatization --> ['lol', 'always', 'convincing']\n",
      "\n",
      "word Tokenization --> ['did', 'you', 'catch', 'the', 'bus', 'are', 'you', 'frying', 'an', 'egg', 'did', 'you', 'make', 'a', 'tea', 'are', 'you', 'eating', 'your', 'mom', 's', 'left', 'over', 'dinner', 'do', 'you', 'feel', 'my', 'love']\n",
      "lemmatization --> ['catch', 'bus', 'frying', 'egg', 'make', 'tea', 'eating', 'mom', 'left', 'dinner', 'feel', 'love']\n",
      "\n",
      "word Tokenization --> ['i', 'm', 'back', 'amp', 'we', 're', 'packing', 'the', 'car', 'now', 'i', 'll', 'let', 'you', 'know', 'if', 'there', 's', 'room']\n",
      "lemmatization --> ['back', 'amp', 'packing', 'car', 'let', 'know', 'room']\n",
      "\n",
      "word Tokenization --> ['ahhh', 'work', 'i', 'vaguely', 'remember', 'that', 'what', 'does', 'it', 'feel', 'like', 'lol']\n",
      "lemmatization --> ['ahhh', 'work', 'vaguely', 'remember', 'feel', 'like', 'lol']\n",
      "\n",
      "word Tokenization --> ['wait', 'that', 's', 'still', 'not', 'all', 'that', 'clear', 'were', 'you', 'not', 'sure', 'about', 'me', 'being', 'sarcastic', 'or', 'that', 'that', 's', 'why', 'x', 'doesn', 't', 'want', 'to', 'live', 'with', 'us']\n",
      "lemmatization --> ['wait', 'still', 'clear', 'sure', 'sarcastic', 'x', 'want', 'live', 'u']\n",
      "\n",
      "word Tokenization --> ['yeah', 'he', 'got', 'in', 'at', 'and', 'was', 'v', 'apologetic', 'n', 'had', 'fallen', 'out', 'and', 'she', 'was', 'actin', 'like', 'spoilt', 'child', 'and', 'he', 'got', 'caught', 'up', 'in', 'that', 'till', 'but', 'we', 'won', 't', 'go', 'there', 'not', 'doing', 'too', 'badly', 'cheers', 'you']\n",
      "lemmatization --> ['yeah', 'got', 'v', 'apologetic', 'n', 'fallen', 'actin', 'like', 'spoilt', 'child', 'got', 'caught', 'till', 'go', 'badly', 'cheer']\n",
      "\n",
      "word Tokenization --> ['k', 'tell', 'me', 'anything', 'about', 'you']\n",
      "lemmatization --> ['k', 'tell', 'anything']\n",
      "\n",
      "word Tokenization --> ['for', 'fear', 'of', 'fainting', 'with', 'the', 'of', 'all', 'that', 'housework', 'you', 'just', 'did', 'quick', 'have', 'a', 'cuppa']\n",
      "lemmatization --> ['fear', 'fainting', 'housework', 'quick', 'cuppa']\n",
      "\n",
      "word Tokenization --> ['thanks', 'for', 'your', 'subscription', 'to', 'ringtone', 'uk', 'your', 'mobile', 'will', 'be', 'charged', 'month', 'please', 'confirm', 'by', 'replying', 'yes', 'or', 'no', 'if', 'you', 'reply', 'no', 'you', 'will', 'not', 'be', 'charged']\n",
      "lemmatization --> ['thanks', 'subscription', 'ringtone', 'uk', 'mobile', 'charged', 'month', 'please', 'confirm', 'replying', 'yes', 'reply', 'charged']\n",
      "\n",
      "word Tokenization --> ['yup', 'ok', 'i', 'go', 'home', 'look', 'at', 'the', 'timings', 'then', 'i', 'msg', 'again', 'xuhui', 'going', 'to', 'learn', 'on', 'nd', 'may', 'too', 'but', 'her', 'lesson', 'is', 'at', 'am']\n",
      "lemmatization --> ['yup', 'ok', 'go', 'home', 'look', 'timing', 'msg', 'xuhui', 'going', 'learn', 'nd', 'may', 'lesson']\n",
      "\n",
      "word Tokenization --> ['oops', 'i', 'll', 'let', 'you', 'know', 'when', 'my', 'roommate', 's', 'done']\n",
      "lemmatization --> ['oops', 'let', 'know', 'roommate', 'done']\n",
      "\n",
      "word Tokenization --> ['i', 'see', 'the', 'letter', 'b', 'on', 'my', 'car']\n",
      "lemmatization --> ['see', 'letter', 'b', 'car']\n",
      "\n",
      "word Tokenization --> ['anything', 'lor', 'u', 'decide']\n",
      "lemmatization --> ['anything', 'lor', 'u', 'decide']\n",
      "\n",
      "word Tokenization --> ['hello', 'how', 's', 'you', 'and', 'how', 'did', 'saturday', 'go', 'i', 'was', 'just', 'texting', 'to', 'see', 'if', 'you', 'd', 'decided', 'to', 'do', 'anything', 'tomo', 'not', 'that', 'i', 'm', 'trying', 'to', 'invite', 'myself', 'or', 'anything']\n",
      "lemmatization --> ['hello', 'saturday', 'go', 'texting', 'see', 'decided', 'anything', 'tomo', 'trying', 'invite', 'anything']\n",
      "\n",
      "word Tokenization --> ['pls', 'go', 'ahead', 'with', 'watts', 'i', 'just', 'wanted', 'to', 'be', 'sure', 'do', 'have', 'a', 'great', 'weekend', 'abiola']\n",
      "lemmatization --> ['pls', 'go', 'ahead', 'watt', 'wanted', 'sure', 'great', 'weekend', 'abiola']\n",
      "\n",
      "word Tokenization --> ['did', 'i', 'forget', 'to', 'tell', 'you', 'i', 'want', 'you', 'i', 'need', 'you', 'i', 'crave', 'you', 'but', 'most', 'of', 'all', 'i', 'love', 'you', 'my', 'sweet', 'arabian', 'steed', 'mmmmmm', 'yummy']\n",
      "lemmatization --> ['forget', 'tell', 'want', 'need', 'crave', 'love', 'sweet', 'arabian', 'steed', 'mmmmmm', 'yummy']\n",
      "\n",
      "word Tokenization --> ['rodger', 'burns', 'msg', 'we', 'tried', 'to', 'call', 'you', 're', 'your', 'reply', 'to', 'our', 'sms', 'for', 'a', 'free', 'nokia', 'mobile', 'free', 'camcorder', 'please', 'call', 'now', 'for', 'delivery', 'tomorrow']\n",
      "lemmatization --> ['rodger', 'burn', 'msg', 'tried', 'call', 'reply', 'sm', 'free', 'nokia', 'mobile', 'free', 'camcorder', 'please', 'call', 'delivery', 'tomorrow']\n",
      "\n",
      "word Tokenization --> ['who', 'are', 'you', 'seeing']\n",
      "lemmatization --> ['seeing']\n",
      "\n",
      "word Tokenization --> ['great', 'i', 'hope', 'you', 'like', 'your', 'man', 'well', 'endowed', 'i', 'am', 'lt', 'gt', 'inches']\n",
      "lemmatization --> ['great', 'hope', 'like', 'man', 'well', 'endowed', 'lt', 'gt', 'inch']\n",
      "\n",
      "word Tokenization --> ['no', 'calls', 'messages', 'missed', 'calls']\n",
      "lemmatization --> ['call', 'message', 'missed', 'call']\n",
      "\n",
      "word Tokenization --> ['didn', 't', 'you', 'get', 'hep', 'b', 'immunisation', 'in', 'nigeria']\n",
      "lemmatization --> ['get', 'hep', 'b', 'immunisation', 'nigeria']\n",
      "\n",
      "word Tokenization --> ['fair', 'enough', 'anything', 'going', 'on']\n",
      "lemmatization --> ['fair', 'enough', 'anything', 'going']\n",
      "\n",
      "word Tokenization --> ['yeah', 'hopefully', 'if', 'tyler', 'can', 't', 'do', 'it', 'i', 'could', 'maybe', 'ask', 'around', 'a', 'bit']\n",
      "lemmatization --> ['yeah', 'hopefully', 'tyler', 'could', 'maybe', 'ask', 'around', 'bit']\n",
      "\n",
      "word Tokenization --> ['u', 'don', 't', 'know', 'how', 'stubborn', 'i', 'am', 'i', 'didn', 't', 'even', 'want', 'to', 'go', 'to', 'the', 'hospital', 'i', 'kept', 'telling', 'mark', 'i', 'm', 'not', 'a', 'weak', 'sucker', 'hospitals', 'are', 'for', 'weak', 'suckers']\n",
      "lemmatization --> ['u', 'know', 'stubborn', 'even', 'want', 'go', 'hospital', 'kept', 'telling', 'mark', 'weak', 'sucker', 'hospital', 'weak', 'sucker']\n",
      "\n",
      "word Tokenization --> ['what', 'you', 'thinked', 'about', 'me', 'first', 'time', 'you', 'saw', 'me', 'in', 'class']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization --> ['thinked', 'first', 'time', 'saw', 'class']\n",
      "\n",
      "word Tokenization --> ['a', 'gram', 'usually', 'runs', 'like', 'lt', 'gt', 'a', 'half', 'eighth', 'is', 'smarter', 'though', 'and', 'gets', 'you', 'almost', 'a', 'whole', 'second', 'gram', 'for', 'lt', 'gt']\n",
      "lemmatization --> ['gram', 'usually', 'run', 'like', 'lt', 'gt', 'half', 'eighth', 'smarter', 'though', 'get', 'almost', 'whole', 'second', 'gram', 'lt', 'gt']\n",
      "\n",
      "word Tokenization --> ['k', 'fyi', 'x', 'has', 'a', 'ride', 'early', 'tomorrow', 'morning', 'but', 'he', 's', 'crashing', 'at', 'our', 'place', 'tonight']\n",
      "lemmatization --> ['k', 'fyi', 'x', 'ride', 'early', 'tomorrow', 'morning', 'crashing', 'place', 'tonight']\n",
      "\n",
      "word Tokenization --> ['wow', 'i', 'never', 'realized', 'that', 'you', 'were', 'so', 'embarassed', 'by', 'your', 'accomodations', 'i', 'thought', 'you', 'liked', 'it', 'since', 'i', 'was', 'doing', 'the', 'best', 'i', 'could', 'and', 'you', 'always', 'seemed', 'so', 'happy', 'about', 'the', 'cave', 'i', 'm', 'sorry', 'i', 'didn', 't', 'and', 'don', 't', 'have', 'more', 'to', 'give', 'i', 'm', 'sorry', 'i', 'offered', 'i', 'm', 'sorry', 'your', 'room', 'was', 'so', 'embarassing']\n",
      "lemmatization --> ['wow', 'never', 'realized', 'embarassed', 'accomodations', 'thought', 'liked', 'since', 'best', 'could', 'always', 'seemed', 'happy', 'cave', 'sorry', 'give', 'sorry', 'offered', 'sorry', 'room', 'embarassing']\n",
      "\n",
      "word Tokenization --> ['sms', 'ac', 'sptv', 'the', 'new', 'jersey', 'devils', 'and', 'the', 'detroit', 'red', 'wings', 'play', 'ice', 'hockey', 'correct', 'or', 'incorrect', 'end', 'reply', 'end', 'sptv']\n",
      "lemmatization --> ['sm', 'ac', 'sptv', 'new', 'jersey', 'devil', 'detroit', 'red', 'wing', 'play', 'ice', 'hockey', 'correct', 'incorrect', 'end', 'reply', 'end', 'sptv']\n",
      "\n",
      "word Tokenization --> ['do', 'you', 'know', 'what', 'mallika', 'sherawat', 'did', 'yesterday', 'find', 'out', 'now', 'lt', 'url', 'gt']\n",
      "lemmatization --> ['know', 'mallika', 'sherawat', 'yesterday', 'find', 'lt', 'url', 'gt']\n",
      "\n",
      "word Tokenization --> ['congrats', 'year', 'special', 'cinema', 'pass', 'for', 'is', 'yours', 'call', 'now', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'all', 'free', 'bx', 'ip', 'we', 'pm', 'dont', 'miss', 'out']\n",
      "lemmatization --> ['congrats', 'year', 'special', 'cinema', 'pas', 'call', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'free', 'bx', 'ip', 'pm', 'dont', 'miss']\n",
      "\n",
      "word Tokenization --> ['sorry', 'i', 'll', 'call', 'later', 'in', 'meeting']\n",
      "lemmatization --> ['sorry', 'call', 'later', 'meeting']\n",
      "\n",
      "word Tokenization --> ['tell', 'where', 'you', 'reached']\n",
      "lemmatization --> ['tell', 'reached']\n",
      "\n",
      "word Tokenization --> ['yes', 'gauti', 'and', 'sehwag', 'out', 'of', 'odi', 'series']\n",
      "lemmatization --> ['yes', 'gauti', 'sehwag', 'odi', 'series']\n",
      "\n",
      "word Tokenization --> ['your', 'gon', 'na', 'have', 'to', 'pick', 'up', 'a', 'burger', 'for', 'yourself', 'on', 'your', 'way', 'home', 'i', 'can', 't', 'even', 'move', 'pain', 'is', 'killing', 'me']\n",
      "lemmatization --> ['gon', 'na', 'pick', 'burger', 'way', 'home', 'even', 'move', 'pain', 'killing']\n",
      "\n",
      "word Tokenization --> ['ha', 'ha', 'ha', 'good', 'joke', 'girls', 'are', 'situation', 'seekers']\n",
      "lemmatization --> ['ha', 'ha', 'ha', 'good', 'joke', 'girl', 'situation', 'seeker']\n",
      "\n",
      "word Tokenization --> ['its', 'a', 'part', 'of', 'checking', 'iq']\n",
      "lemmatization --> ['part', 'checking', 'iq']\n",
      "\n",
      "word Tokenization --> ['sorry', 'my', 'roommates', 'took', 'forever', 'it', 'ok', 'if', 'i', 'come', 'by', 'now']\n",
      "lemmatization --> ['sorry', 'roommate', 'took', 'forever', 'ok', 'come']\n",
      "\n",
      "word Tokenization --> ['ok', 'lar', 'i', 'double', 'check', 'wif', 'da', 'hair', 'dresser', 'already', 'he', 'said', 'wun', 'cut', 'v', 'short', 'he', 'said', 'will', 'cut', 'until', 'i', 'look', 'nice']\n",
      "lemmatization --> ['ok', 'lar', 'double', 'check', 'wif', 'da', 'hair', 'dresser', 'already', 'said', 'wun', 'cut', 'v', 'short', 'said', 'cut', 'look', 'nice']\n",
      "\n",
      "word Tokenization --> ['as', 'a', 'valued', 'customer', 'i', 'am', 'pleased', 'to', 'advise', 'you', 'that', 'following', 'recent', 'review', 'of', 'your', 'mob', 'no', 'you', 'are', 'awarded', 'with', 'a', 'bonus', 'prize', 'call']\n",
      "lemmatization --> ['valued', 'customer', 'pleased', 'advise', 'following', 'recent', 'review', 'mob', 'awarded', 'bonus', 'prize', 'call']\n",
      "\n",
      "word Tokenization --> ['today', 'is', 'song', 'dedicated', 'day', 'which', 'song', 'will', 'u', 'dedicate', 'for', 'me', 'send', 'this', 'to', 'all', 'ur', 'valuable', 'frnds', 'but', 'first', 'rply', 'me']\n",
      "lemmatization --> ['today', 'song', 'dedicated', 'day', 'song', 'u', 'dedicate', 'send', 'ur', 'valuable', 'frnds', 'first', 'rply']\n",
      "\n",
      "word Tokenization --> ['urgent', 'ur', 'awarded', 'a', 'complimentary', 'trip', 'to', 'eurodisinc', 'trav', 'aco', 'entry', 'or', 'to', 'claim', 'txt', 'dis', 'to', 'morefrmmob', 'shracomorsglsuplt', 'ls', 'aj']\n",
      "lemmatization --> ['urgent', 'ur', 'awarded', 'complimentary', 'trip', 'eurodisinc', 'trav', 'aco', 'entry', 'claim', 'txt', 'dis', 'morefrmmob', 'shracomorsglsuplt', 'l', 'aj']\n",
      "\n",
      "word Tokenization --> ['did', 'you', 'hear', 'about', 'the', 'new', 'divorce', 'barbie', 'it', 'comes', 'with', 'all', 'of', 'ken', 's', 'stuff']\n",
      "lemmatization --> ['hear', 'new', 'divorce', 'barbie', 'come', 'ken', 'stuff']\n",
      "\n",
      "word Tokenization --> ['i', 'plane', 'to', 'give', 'on', 'this', 'month', 'end']\n",
      "lemmatization --> ['plane', 'give', 'month', 'end']\n",
      "\n",
      "word Tokenization --> ['wah', 'lucky', 'man', 'then', 'can', 'save', 'money', 'hee']\n",
      "lemmatization --> ['wah', 'lucky', 'man', 'save', 'money', 'hee']\n",
      "\n",
      "word Tokenization --> ['finished', 'class', 'where', 'are', 'you']\n",
      "lemmatization --> ['finished', 'class']\n",
      "\n",
      "word Tokenization --> ['hi', 'babe', 'im', 'at', 'home', 'now', 'wan', 'na', 'do', 'something', 'xx']\n",
      "lemmatization --> ['hi', 'babe', 'im', 'home', 'wan', 'na', 'something', 'xx']\n",
      "\n",
      "word Tokenization --> ['k', 'k', 'where', 'are', 'you', 'how', 'did', 'you', 'performed']\n",
      "lemmatization --> ['k', 'k', 'performed']\n",
      "\n",
      "word Tokenization --> ['u', 'can', 'call', 'me', 'now']\n",
      "lemmatization --> ['u', 'call']\n",
      "\n",
      "word Tokenization --> ['i', 'am', 'waiting', 'machan', 'call', 'me', 'once', 'you', 'free']\n",
      "lemmatization --> ['waiting', 'machan', 'call', 'free']\n",
      "\n",
      "word Tokenization --> ['thats', 'cool', 'i', 'am', 'a', 'gentleman', 'and', 'will', 'treat', 'you', 'with', 'dignity', 'and', 'respect']\n",
      "lemmatization --> ['thats', 'cool', 'gentleman', 'treat', 'dignity', 'respect']\n",
      "\n",
      "word Tokenization --> ['i', 'like', 'you', 'peoples', 'very', 'much', 'but', 'am', 'very', 'shy', 'pa']\n",
      "lemmatization --> ['like', 'people', 'much', 'shy', 'pa']\n",
      "\n",
      "word Tokenization --> ['does', 'not', 'operate', 'after', 'lt', 'gt', 'or', 'what']\n",
      "lemmatization --> ['operate', 'lt', 'gt']\n",
      "\n",
      "word Tokenization --> ['its', 'not', 'the', 'same', 'here', 'still', 'looking', 'for', 'a', 'job', 'how', 'much', 'do', 'ta', 's', 'earn', 'there']\n",
      "lemmatization --> ['still', 'looking', 'job', 'much', 'ta', 'earn']\n",
      "\n",
      "word Tokenization --> ['sorry', 'i', 'll', 'call', 'later']\n",
      "lemmatization --> ['sorry', 'call', 'later']\n",
      "\n",
      "word Tokenization --> ['k', 'did', 'you', 'call', 'me', 'just', 'now', 'ah']\n",
      "lemmatization --> ['k', 'call', 'ah']\n",
      "\n",
      "word Tokenization --> ['ok', 'i', 'am', 'on', 'the', 'way', 'to', 'home', 'hi', 'hi']\n",
      "lemmatization --> ['ok', 'way', 'home', 'hi', 'hi']\n",
      "\n",
      "word Tokenization --> ['you', 'will', 'be', 'in', 'the', 'place', 'of', 'that', 'man']\n",
      "lemmatization --> ['place', 'man']\n",
      "\n",
      "word Tokenization --> ['yup', 'next', 'stop']\n",
      "lemmatization --> ['yup', 'next', 'stop']\n",
      "\n",
      "word Tokenization --> ['i', 'call', 'you', 'later', 'don', 't', 'have', 'network', 'if', 'urgnt', 'sms', 'me']\n",
      "lemmatization --> ['call', 'later', 'network', 'urgnt', 'sm']\n",
      "\n",
      "word Tokenization --> ['for', 'real', 'when', 'u', 'getting', 'on', 'yo', 'i', 'only', 'need', 'more', 'tickets', 'and', 'one', 'more', 'jacket', 'and', 'i', 'm', 'done', 'i', 'already', 'used', 'all', 'my', 'multis']\n",
      "lemmatization --> ['real', 'u', 'getting', 'yo', 'need', 'ticket', 'one', 'jacket', 'done', 'already', 'used', 'multis']\n",
      "\n",
      "word Tokenization --> ['yes', 'i', 'started', 'to', 'send', 'requests', 'to', 'make', 'it', 'but', 'pain', 'came', 'back', 'so', 'i', 'm', 'back', 'in', 'bed', 'double', 'coins', 'at', 'the', 'factory', 'too', 'i', 'got', 'ta', 'cash', 'in', 'all', 'my', 'nitros']\n",
      "lemmatization --> ['yes', 'started', 'send', 'request', 'make', 'pain', 'came', 'back', 'back', 'bed', 'double', 'coin', 'factory', 'got', 'ta', 'cash', 'nitros']\n",
      "\n",
      "word Tokenization --> ['i', 'm', 'really', 'not', 'up', 'to', 'it', 'still', 'tonight', 'babe']\n",
      "lemmatization --> ['really', 'still', 'tonight', 'babe']\n",
      "\n",
      "word Tokenization --> ['ela', 'kano', 'il', 'download', 'come', 'wen', 'ur', 'free']\n",
      "lemmatization --> ['ela', 'kano', 'il', 'download', 'come', 'wen', 'ur', 'free']\n",
      "\n",
      "word Tokenization --> ['yeah', 'do', 'don', 't', 'stand', 'to', 'close', 'tho', 'you', 'll', 'catch', 'something']\n",
      "lemmatization --> ['yeah', 'stand', 'close', 'tho', 'catch', 'something']\n",
      "\n",
      "word Tokenization --> ['sorry', 'to', 'be', 'a', 'pain', 'is', 'it', 'ok', 'if', 'we', 'meet', 'another', 'night', 'i', 'spent', 'late', 'afternoon', 'in', 'casualty', 'and', 'that', 'means', 'i', 'haven', 't', 'done', 'any', 'of', 'y', 'stuff', 'moro', 'and', 'that', 'includes', 'all', 'my', 'time', 'sheets', 'and', 'that', 'sorry']\n",
      "lemmatization --> ['sorry', 'pain', 'ok', 'meet', 'another', 'night', 'spent', 'late', 'afternoon', 'casualty', 'mean', 'done', 'stuff', 'moro', 'includes', 'time', 'sheet', 'sorry']\n",
      "\n",
      "word Tokenization --> ['smile', 'in', 'pleasure', 'smile', 'in', 'pain', 'smile', 'when', 'trouble', 'pours', 'like', 'rain', 'smile', 'when', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'to', 'see', 'u', 'smiling']\n",
      "lemmatization --> ['smile', 'pleasure', 'smile', 'pain', 'smile', 'trouble', 'pours', 'like', 'rain', 'smile', 'sum', 'hurt', 'u', 'smile', 'becoz', 'someone', 'still', 'love', 'see', 'u', 'smiling']\n",
      "\n",
      "word Tokenization --> ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "lemmatization --> ['please', 'call', 'customer', 'service', 'representative', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "\n",
      "word Tokenization --> ['havent', 'planning', 'to', 'buy', 'later', 'i', 'check', 'already', 'lido', 'only', 'got', 'show', 'in', 'e', 'afternoon', 'u', 'finish', 'work', 'already']\n",
      "lemmatization --> ['havent', 'planning', 'buy', 'later', 'check', 'already', 'lido', 'got', 'show', 'e', 'afternoon', 'u', 'finish', 'work', 'already']\n",
      "\n",
      "word Tokenization --> ['your', 'free', 'ringtone', 'is', 'waiting', 'to', 'be', 'collected', 'simply', 'text', 'the', 'password', 'mix', 'to', 'to', 'verify', 'get', 'usher', 'and', 'britney', 'fml', 'po', 'box', 'mk', 'h', 'ppw']\n",
      "lemmatization --> ['free', 'ringtone', 'waiting', 'collected', 'simply', 'text', 'password', 'mix', 'verify', 'get', 'usher', 'britney', 'fml', 'po', 'box', 'mk', 'h', 'ppw']\n",
      "\n",
      "word Tokenization --> ['watching', 'telugu', 'movie', 'wat', 'abt', 'u']\n",
      "lemmatization --> ['watching', 'telugu', 'movie', 'wat', 'abt', 'u']\n",
      "\n",
      "word Tokenization --> ['i', 'see', 'when', 'we', 'finish', 'we', 'have', 'loads', 'of', 'loans', 'to', 'pay']\n",
      "lemmatization --> ['see', 'finish', 'load', 'loan', 'pay']\n",
      "\n",
      "word Tokenization --> ['hi', 'wk', 'been', 'ok', 'on', 'hols', 'now', 'yes', 'on', 'for', 'a', 'bit', 'of', 'a', 'run', 'forgot', 'that', 'i', 'have', 'hairdressers', 'appointment', 'at', 'four', 'so', 'need', 'to', 'get', 'home', 'n', 'shower', 'beforehand', 'does', 'that', 'cause', 'prob', 'for', 'u']\n",
      "lemmatization --> ['hi', 'wk', 'ok', 'hols', 'yes', 'bit', 'run', 'forgot', 'hairdresser', 'appointment', 'four', 'need', 'get', 'home', 'n', 'shower', 'beforehand', 'cause', 'prob', 'u']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Steps\n",
    "# 1. Remove special charecters and numbers.\n",
    "# 2. lower case.\n",
    "# 3. Word tokenization or split the word.\n",
    "# 4. stemming or lemmatization.\n",
    "\n",
    "corpus=[]\n",
    "for i in range(len(df)):\n",
    "      a=df['Messages'][i]\n",
    "      doc=re.sub('[^a-zA-Z]',' ',a)\n",
    "      doc=doc.lower()\n",
    "      doc=nltk.word_tokenize(doc)\n",
    "      print('word Tokenization -->',doc)\n",
    "#       We can use either stemming or lemmatization\n",
    "#       1. stemming\n",
    "#       doc=[ps.stem(j) for j in doc if not j in stopwords.words('english')]\n",
    "#       print('stemming -->',doc)\n",
    "#       2. lemmatization\n",
    "      doc=[lemma.lemmatize(j) for j in doc if not j in stopwords.words('english')]\n",
    "      print('lemmatization -->',doc)\n",
    "      print()\n",
    "      doc=' '.join(doc)\n",
    "      corpus.append(doc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Subir\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Bag of words\n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=500)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "y=pd.get_dummies(df['Labels'],drop_first=True)\n",
    "\n",
    "# Train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=0)\n",
    "\n",
    "# Model\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam=MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "y_pred=spam.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "con=confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
